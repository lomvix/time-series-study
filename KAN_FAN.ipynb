{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KANLinear(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        grid_size=5,\n",
    "        spline_order=3,\n",
    "        scale_noise=0.1,\n",
    "        scale_base=1.0,\n",
    "        scale_spline=1.0,\n",
    "        enable_standalone_scale_spline=True,\n",
    "        base_activation=torch.nn.SiLU,\n",
    "        grid_eps=0.02,\n",
    "        grid_range=[-1, 1],\n",
    "    ):\n",
    "        super(KANLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "\n",
    "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
    "        grid = (\n",
    "            (\n",
    "                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n",
    "                + grid_range[0]\n",
    "            )\n",
    "            .expand(in_features, -1)\n",
    "            .contiguous()\n",
    "        )\n",
    "        self.register_buffer(\"grid\", grid)\n",
    "\n",
    "        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.spline_weight = torch.nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features, grid_size + spline_order)\n",
    "        )\n",
    "        if enable_standalone_scale_spline:\n",
    "            self.spline_scaler = torch.nn.Parameter(\n",
    "                torch.Tensor(out_features, in_features)\n",
    "            )\n",
    "\n",
    "        self.scale_noise = scale_noise\n",
    "        self.scale_base = scale_base\n",
    "        self.scale_spline = scale_spline\n",
    "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
    "        self.base_activation = base_activation()\n",
    "        self.grid_eps = grid_eps\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n",
    "        with torch.no_grad():\n",
    "            noise = (\n",
    "                (\n",
    "                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n",
    "                    - 1 / 2\n",
    "                )\n",
    "                * self.scale_noise\n",
    "                / self.grid_size\n",
    "            )\n",
    "            self.spline_weight.data.copy_(\n",
    "                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n",
    "                * self.curve2coeff(\n",
    "                    self.grid.T[self.spline_order : -self.spline_order],\n",
    "                    noise,\n",
    "                )\n",
    "            )\n",
    "            if self.enable_standalone_scale_spline:\n",
    "                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n",
    "                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n",
    "\n",
    "    def b_splines(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the B-spline bases for the given input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "\n",
    "        grid: torch.Tensor = (\n",
    "            self.grid\n",
    "        )  # (in_features, grid_size + 2 * spline_order + 1)\n",
    "        x = x.unsqueeze(-1)\n",
    "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
    "        for k in range(1, self.spline_order + 1):\n",
    "            bases = (\n",
    "                (x - grid[:, : -(k + 1)])\n",
    "                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n",
    "                * bases[:, :, :-1]\n",
    "            ) + (\n",
    "                (grid[:, k + 1 :] - x)\n",
    "                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n",
    "                * bases[:, :, 1:]\n",
    "            )\n",
    "\n",
    "        assert bases.size() == (\n",
    "            x.size(0),\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return bases.contiguous()\n",
    "\n",
    "    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the coefficients of the curve that interpolates the given points.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        assert y.size() == (x.size(0), self.in_features, self.out_features)\n",
    "\n",
    "        A = self.b_splines(x).transpose(\n",
    "            0, 1\n",
    "        )  # (in_features, batch_size, grid_size + spline_order)\n",
    "        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n",
    "        solution = torch.linalg.lstsq(\n",
    "            A, B\n",
    "        ).solution  # (in_features, grid_size + spline_order, out_features)\n",
    "        result = solution.permute(\n",
    "            2, 0, 1\n",
    "        )  # (out_features, in_features, grid_size + spline_order)\n",
    "\n",
    "        assert result.size() == (\n",
    "            self.out_features,\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return result.contiguous()\n",
    "\n",
    "    @property\n",
    "    def scaled_spline_weight(self):\n",
    "        return self.spline_weight * (\n",
    "            self.spline_scaler.unsqueeze(-1)\n",
    "            if self.enable_standalone_scale_spline\n",
    "            else 1.0\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert x.size(-1) == self.in_features\n",
    "        original_shape = x.shape\n",
    "        x = x.view(-1, self.in_features)\n",
    "\n",
    "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
    "        spline_output = F.linear(\n",
    "            self.b_splines(x).view(x.size(0), -1),\n",
    "            self.scaled_spline_weight.view(self.out_features, -1),\n",
    "        )\n",
    "        output = base_output + spline_output\n",
    "        \n",
    "        output = output.view(*original_shape[:-1], self.out_features)\n",
    "        return output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_grid(self, x: torch.Tensor, margin=0.01):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        batch = x.size(0)\n",
    "\n",
    "        splines = self.b_splines(x)  # (batch, in, coeff)\n",
    "        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n",
    "        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n",
    "        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n",
    "        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)\n",
    "        unreduced_spline_output = unreduced_spline_output.permute(\n",
    "            1, 0, 2\n",
    "        )  # (batch, in, out)\n",
    "\n",
    "        # sort each channel individually to collect data distribution\n",
    "        x_sorted = torch.sort(x, dim=0)[0]\n",
    "        grid_adaptive = x_sorted[\n",
    "            torch.linspace(\n",
    "                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n",
    "        grid_uniform = (\n",
    "            torch.arange(\n",
    "                self.grid_size + 1, dtype=torch.float32, device=x.device\n",
    "            ).unsqueeze(1)\n",
    "            * uniform_step\n",
    "            + x_sorted[0]\n",
    "            - margin\n",
    "        )\n",
    "\n",
    "        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n",
    "        grid = torch.concatenate(\n",
    "            [\n",
    "                grid[:1]\n",
    "                - uniform_step\n",
    "                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n",
    "                grid,\n",
    "                grid[-1:]\n",
    "                + uniform_step\n",
    "                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        self.grid.copy_(grid.T)\n",
    "        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        \"\"\"\n",
    "        Compute the regularization loss.\n",
    "\n",
    "        This is a dumb simulation of the original L1 regularization as stated in the\n",
    "        paper, since the original one requires computing absolutes and entropy from the\n",
    "        expanded (batch, in_features, out_features) intermediate tensor, which is hidden\n",
    "        behind the F.linear function if we want an memory efficient implementation.\n",
    "\n",
    "        The L1 regularization is now computed as mean absolute value of the spline\n",
    "        weights. The authors implementation also includes this term in addition to the\n",
    "        sample-based regularization.\n",
    "        \"\"\"\n",
    "        l1_fake = self.spline_weight.abs().mean(-1)\n",
    "        regularization_loss_activation = l1_fake.sum()\n",
    "        p = l1_fake / regularization_loss_activation\n",
    "        regularization_loss_entropy = -torch.sum(p * p.log())\n",
    "        return (\n",
    "            regularize_activation * regularization_loss_activation\n",
    "            + regularize_entropy * regularization_loss_entropy\n",
    "        )\n",
    "\n",
    "\n",
    "class KAN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers_hidden,\n",
    "        grid_size=5,\n",
    "        spline_order=3,\n",
    "        scale_noise=0.1,\n",
    "        scale_base=1.0,\n",
    "        scale_spline=1.0,\n",
    "        base_activation=torch.nn.SiLU,\n",
    "        grid_eps=0.02,\n",
    "        grid_range=[-1, 1],\n",
    "    ):\n",
    "        super(KAN, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for in_features, out_features in zip(layers_hidden, layers_hidden[1:]):\n",
    "            self.layers.append(\n",
    "                KANLinear(\n",
    "                    in_features,\n",
    "                    out_features,\n",
    "                    grid_size=grid_size,\n",
    "                    spline_order=spline_order,\n",
    "                    scale_noise=scale_noise,\n",
    "                    scale_base=scale_base,\n",
    "                    scale_spline=scale_spline,\n",
    "                    base_activation=base_activation,\n",
    "                    grid_eps=grid_eps,\n",
    "                    grid_range=grid_range,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, update_grid=False):\n",
    "        for layer in self.layers:\n",
    "            if update_grid:\n",
    "                layer.update_grid(x)\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        return sum(\n",
    "            layer.regularization_loss(regularize_activation, regularize_entropy)\n",
    "            for layer in self.layers\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FANLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    FANLayer: The layer used in FAN (https://arxiv.org/abs/2410.02675).\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): The number of input features.\n",
    "        output_dim (int): The number of output features.\n",
    "        p_ratio (float): The ratio of output dimensions used for cosine and sine parts (default: 0.25).\n",
    "        activation (str or callable): The activation function to apply to the g component. If a string is passed,\n",
    "            the corresponding activation from torch.nn.functional is used (default: 'gelu').\n",
    "        use_p_bias (bool): If True, include bias in the linear transformations of p component (default: True). \n",
    "            There is almost no difference between bias and non-bias in our experiments.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, p_ratio=0.25, activation='gelu', use_p_bias=True):\n",
    "        super(FANLayer, self).__init__()\n",
    "        \n",
    "        # Ensure the p_ratio is within a valid range\n",
    "        assert 0 < p_ratio < 0.5, \"p_ratio must be between 0 and 0.5\"\n",
    "        \n",
    "        self.p_ratio = p_ratio\n",
    "        p_output_dim = int(output_dim * self.p_ratio)\n",
    "        g_output_dim = output_dim - p_output_dim * 2  # Account for cosine and sine terms\n",
    "\n",
    "        # Linear transformation for the p component (for cosine and sine parts)\n",
    "        self.input_linear_p = nn.Linear(input_dim, p_output_dim, bias=use_p_bias)\n",
    "        \n",
    "        # Linear transformation for the g component\n",
    "        self.input_linear_g = nn.Linear(input_dim, g_output_dim)\n",
    "        \n",
    "        # Set the activation function\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = getattr(F, activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src (Tensor): Input tensor of shape (batch_size, input_dim).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, output_dim), after applying the FAN layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Apply the linear transformation followed by the activation for the g component\n",
    "        g = self.activation(self.input_linear_g(src))\n",
    "        \n",
    "        # Apply the linear transformation for the p component\n",
    "        p = self.input_linear_p(src)\n",
    "        \n",
    "        # Concatenate cos(p), sin(p), and activated g along the last dimension\n",
    "        output = torch.cat((torch.cos(p), torch.sin(p), g), dim=-1)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\E'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\E'\n",
      "C:\\Users\\26921\\AppData\\Local\\Temp\\ipykernel_25104\\1029028916.py:1: SyntaxWarning: invalid escape sequence '\\E'\n",
      "  data = pd.read_csv(\"data\\ETTh.csv\")\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data\\ETTh.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,1:-1]\n",
    "y = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = MinMaxScaler().fit_transform(X)\n",
    "y = MinMaxScaler().fit_transform(np.array(y).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test = X[:int(len(X)*0.7)],X[int(len(X)*0.7):]\n",
    "y_train,y_test = y[:int(len(y)*0.7)],y[int(len(y)*0.7):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(np.array(x_train),dtype=torch.float32), torch.tensor(np.array(y_train),dtype=torch.float32))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(torch.tensor(np.array(x_test),dtype=torch.float32), torch.tensor(np.array(y_test),dtype=torch.float32))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "train_data = train_loader\n",
    "test_data = test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = X.shape[1]  # 设置特征数目\n",
    "out_prediction = 1  # 设置输出数目\n",
    "learning_rate = 0.01  # 设置学习率\n",
    "epochs = 20  # 设置训练代数\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output, n_neuron1, n_neuron2,n_layer):  # n_feature为特征数目，这个数字不能随便取,n_output为特征对应的输出数目，也不能随便取\n",
    "        self.n_feature=n_feature\n",
    "        self.n_output=n_output\n",
    "        self.n_neuron1=n_neuron1\n",
    "        self.n_neuron2=n_neuron2\n",
    "        self.n_layer=n_layer\n",
    "        super(Net, self).__init__()\n",
    "        self.input_layer = nn.Linear(self.n_feature, self.n_neuron1) # 输入层\n",
    "        self.hidden1 = nn.Linear(self.n_neuron1, self.n_neuron2) # 1类隐藏层    \n",
    "        self.hidden2 = FANLayer(self.n_neuron2, self.n_neuron2) # 2类隐藏\n",
    "        self.predict = nn.Linear(self.n_neuron2, self.n_output) # 输出层\n",
    " \n",
    "    def forward(self, x):\n",
    "        '''定义前向传递过程'''\n",
    "        out = self.input_layer(x)\n",
    "        out = torch.relu(out) # 使用relu函数非线性激活\n",
    "        out = self.hidden1(out)\n",
    "        out = torch.relu(out)\n",
    "        for i in range(self.n_layer):\n",
    "            out = self.hidden2(out)\n",
    "            out = torch.relu(out)\n",
    "        out = out.float()\n",
    "        out = self.predict( # 回归问题最后一层不需要激活函数\n",
    "            out\n",
    "        )  # 除去feature_number与out_prediction不能随便取，隐藏层数与其他神经元数目均可以适当调整以得到最佳预测效果\n",
    "        return out\n",
    " \n",
    "net = Net(n_feature=feature_number,\n",
    "                      n_output=out_prediction,\n",
    "                      n_layer=1,\n",
    "                      n_neuron1=16,\n",
    "                      n_neuron2=8) # 这里直接确定了隐藏层数目以及神经元数目，实际操作中需要遍历\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)  # 使用Adam算法更新参数\n",
    "criteon = torch.nn.MSELoss()  # 误差计算公式，回归问题采用均方误差\n",
    "\n",
    "for epoch in range(epochs):  # 整个数据集迭代次数\n",
    "    net.train() # 启动训练模式\n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_data):\n",
    "        logits = net.forward(data)  # 前向计算结果(预测结果）\n",
    "        loss = criteon(logits, target)  # 计算损失\n",
    "        optimizer.zero_grad()  # 梯度清零\n",
    "        loss.backward()  # 后向传递过程\n",
    "        optimizer.step()  # 优化权重与偏差矩阵\n",
    "    end_time = time.time()\n",
    "    print(\"epoch\",epoch,' ',\"Traintime :\", end_time - start_time)\n",
    "       \n",
    "    logit = []  # 这个是验证集，可以根据验证集的结果进行调参，这里根据验证集的结果选取最优的神经网络层数与神经元数目\n",
    "    target = []\n",
    "    net.eval() # 启动测试模式\n",
    "    for data, targets in test_data:  # 输出验证集的平均误差\n",
    "        logits = net.forward(data).detach().numpy()\n",
    "        targets=targets.detach().numpy()\n",
    "        target.append(targets[0])\n",
    "        logit.append(logits[0])\n",
    "    average_loss =  criteon(torch.tensor(logit), torch.tensor(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = X.shape[1]  # 设置特征数目\n",
    "out_prediction = 1  # 设置输出数目\n",
    "learning_rate = 0.01  # 设置学习率\n",
    "epochs = 20  # 设置训练代数\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output, n_neuron1, n_neuron2,n_layer):  # n_feature为特征数目，这个数字不能随便取,n_output为特征对应的输出数目，也不能随便取\n",
    "        self.n_feature=n_feature\n",
    "        self.n_output=n_output\n",
    "        self.n_neuron1=n_neuron1\n",
    "        self.n_neuron2=n_neuron2\n",
    "        self.n_layer=n_layer\n",
    "        super(Net, self).__init__()\n",
    "        self.input_layer = nn.Linear(self.n_feature, self.n_neuron1) # 输入层\n",
    "        self.hidden1 = nn.Linear(self.n_neuron1, self.n_neuron2) # 1类隐藏层    \n",
    "        self.hidden2 = KANLinear(self.n_neuron2, self.n_neuron2) # 2类隐藏\n",
    "        self.predict = nn.Linear(self.n_neuron2, self.n_output) # 输出层\n",
    " \n",
    "    def forward(self, x):\n",
    "        '''定义前向传递过程'''\n",
    "        out = self.input_layer(x)\n",
    "        out = torch.relu(out) # 使用relu函数非线性激活\n",
    "        out = self.hidden1(out)\n",
    "        out = torch.relu(out)\n",
    "        for i in range(self.n_layer):\n",
    "            out = self.hidden2(out)\n",
    "            out = torch.relu(out)\n",
    "        out = out.float()\n",
    "        out = self.predict( # 回归问题最后一层不需要激活函数\n",
    "            out\n",
    "        )  # 除去feature_number与out_prediction不能随便取，隐藏层数与其他神经元数目均可以适当调整以得到最佳预测效果\n",
    "        return out\n",
    " \n",
    "net = Net(n_feature=feature_number,\n",
    "                      n_output=out_prediction,\n",
    "                      n_layer=1,\n",
    "                      n_neuron1=16,\n",
    "                      n_neuron2=8) # 这里直接确定了隐藏层数目以及神经元数目，实际操作中需要遍历\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)  # 使用Adam算法更新参数\n",
    "criteon = torch.nn.MSELoss()  # 误差计算公式，回归问题采用均方误差\n",
    "\n",
    "for epoch in range(epochs):  # 整个数据集迭代次数\n",
    "    net.train() # 启动训练模式\n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_data):\n",
    "        logits = net.forward(data)  # 前向计算结果(预测结果）\n",
    "        loss = criteon(logits, target)  # 计算损失\n",
    "        optimizer.zero_grad()  # 梯度清零\n",
    "        loss.backward()  # 后向传递过程\n",
    "        optimizer.step()  # 优化权重与偏差矩阵\n",
    "    end_time = time.time()\n",
    "    print(\"epoch\",epoch,' ',\"Traintime :\", end_time - start_time)\n",
    "       \n",
    "    logit = []  # 这个是验证集，可以根据验证集的结果进行调参，这里根据验证集的结果选取最优的神经网络层数与神经元数目\n",
    "    target = []\n",
    "    net.eval() # 启动测试模式\n",
    "    for data, targets in test_data:  # 输出验证集的平均误差\n",
    "        logits = net.forward(data).detach().numpy()\n",
    "        targets=targets.detach().numpy()\n",
    "        target.append(targets[0])\n",
    "        logit.append(logits[0])\n",
    "    average_loss =  criteon(torch.tensor(logit), torch.tensor(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = X.shape[1]  # 设置特征数目\n",
    "out_prediction = 1  # 设置输出数目\n",
    "learning_rate = 0.01  # 设置学习率\n",
    "epochs = 20  # 设置训练代数\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output, n_neuron1, n_neuron2,n_layer):  # n_feature为特征数目，这个数字不能随便取,n_output为特征对应的输出数目，也不能随便取\n",
    "        self.n_feature=n_feature\n",
    "        self.n_output=n_output\n",
    "        self.n_neuron1=n_neuron1\n",
    "        self.n_neuron2=n_neuron2\n",
    "        self.n_layer=n_layer\n",
    "        super(Net, self).__init__()\n",
    "        self.input_layer = nn.Linear(self.n_feature, self.n_neuron1) # 输入层\n",
    "        self.hidden1 = nn.Linear(self.n_neuron1, self.n_neuron2) # 1类隐藏层    \n",
    "        self.hidden2 = nn.Linear(self.n_neuron2, self.n_neuron2) # 2类隐藏\n",
    "        self.predict = nn.Linear(self.n_neuron2, self.n_output) # 输出层\n",
    " \n",
    "    def forward(self, x):\n",
    "        '''定义前向传递过程'''\n",
    "        out = self.input_layer(x)\n",
    "        out = torch.relu(out) # 使用relu函数非线性激活\n",
    "        out = self.hidden1(out)\n",
    "        out = torch.relu(out)\n",
    "        for i in range(self.n_layer):\n",
    "            out = self.hidden2(out)\n",
    "            out = torch.relu(out)\n",
    "        out = out.float()\n",
    "        out = self.predict( # 回归问题最后一层不需要激活函数\n",
    "            out\n",
    "        )  # 除去feature_number与out_prediction不能随便取，隐藏层数与其他神经元数目均可以适当调整以得到最佳预测效果\n",
    "        return out\n",
    " \n",
    "net = Net(n_feature=feature_number,\n",
    "                      n_output=out_prediction,\n",
    "                      n_layer=1,\n",
    "                      n_neuron1=16,\n",
    "                      n_neuron2=8) # 这里直接确定了隐藏层数目以及神经元数目，实际操作中需要遍历\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)  # 使用Adam算法更新参数\n",
    "criteon = torch.nn.MSELoss()  # 误差计算公式，回归问题采用均方误差\n",
    "\n",
    "for epoch in range(epochs):  # 整个数据集迭代次数\n",
    "    net.train() # 启动训练模式\n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_data):\n",
    "        logits = net.forward(data)  # 前向计算结果(预测结果）\n",
    "        loss = criteon(logits, target)  # 计算损失\n",
    "        optimizer.zero_grad()  # 梯度清零\n",
    "        loss.backward()  # 后向传递过程\n",
    "        optimizer.step()  # 优化权重与偏差矩阵\n",
    "    end_time = time.time()\n",
    "    print(\"epoch\",epoch,' ',\"Traintime :\", end_time - start_time)\n",
    "       \n",
    "    logit = []  # 这个是验证集，可以根据验证集的结果进行调参，这里根据验证集的结果选取最优的神经网络层数与神经元数目\n",
    "    target = []\n",
    "    net.eval() # 启动测试模式\n",
    "    for data, targets in test_data:  # 输出验证集的平均误差\n",
    "        logits = net.forward(data).detach().numpy()\n",
    "        targets=targets.detach().numpy()\n",
    "        target.append(targets[0])\n",
    "        logit.append(logits[0])\n",
    "    average_loss =  criteon(torch.tensor(logit), torch.tensor(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0866)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average_loss\n",
    "\n",
    "1. KAN: 0.0549\n",
    "2. MLP：0.0866\n",
    "3. FAN: 0.1020"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
